{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi_function_definition\n",
    "from collections import defaultdict, Counter\n",
    "from math import log\n",
    "import numpy as np\n",
    "EPSILON = 1e-5\n",
    "\n",
    "def viterbi(train, test):\n",
    "    #1. Count tag, tag pairs, (word, tag) pairs  \n",
    "    EPSILON_LOG = np.log(EPSILON)\n",
    "    initial_counter = Counter() \n",
    "    tag_counter = Counter() \n",
    "    tag_tag_counter = defaultdict(Counter)\n",
    "    word_tag_counter = defaultdict(Counter)\n",
    "\n",
    "    for sentence in train:\n",
    "        initial_counter[sentence[1][1]] += 1\n",
    "        for i in range(len(sentence)-1):\n",
    "            if sentence[i][0] not in [\"START\", \"END\"]:\n",
    "                tag1 = sentence[i][1]\n",
    "                tag2 = sentence[i+1][1]\n",
    "                tag_counter[sentence[i][1]] += 1\n",
    "                word_tag_counter[sentence[i][0]][sentence[i][1]] += 1\n",
    "                tag_tag_counter[tag1][tag2] += 1\n",
    "    \n",
    "    #2. Find each probability and take the log of each probability\n",
    "\n",
    "    #initial probability\n",
    "    total_count_for_initial_counter = sum(initial_counter.values())\n",
    "    initial_probabilities = defaultdict(lambda: defaultdict(float))\n",
    "    for item, count in initial_counter.items():\n",
    "        initial_probabilities[item] = np.log(count/ total_count_for_initial_counter) + EPSILON_LOG\n",
    "\n",
    "    #P(tagB|tagA)\n",
    "    transition_probabilities = defaultdict(lambda: defaultdict(float))\n",
    "    for prior_tag, tag_counts in tag_tag_counter.items():\n",
    "        for tag, count in tag_counts.items():\n",
    "            transition_probabilities[prior_tag][tag] = np.log(count / tag_counter[prior_tag]) + EPSILON_LOG\n",
    "    \n",
    "    #P(word|tag)\n",
    "    emission_probabilities = defaultdict(lambda: defaultdict(float))\n",
    "    for word, tag_counts in word_tag_counter.items():\n",
    "        for tag, count in tag_counts.items():\n",
    "            emission_probabilities[word][tag] = np.log(count / tag_counter[tag]) + EPSILON_LOG\n",
    "\n",
    "    #3. Find the maximum probability path\n",
    "    tagged_sentences = []\n",
    "    for sentence in test:\n",
    "        tag_probability_matrix = {0: {tag: initial_probabilities[tag] for tag in tag_counter}}\n",
    "        reminder = {}\n",
    "\n",
    "        core_sentence = sentence[1:-1]\n",
    "\n",
    "        for i in range(1, len(core_sentence)):\n",
    "            tag_probability_matrix[i] = {}\n",
    "            reminder[i] = {}\n",
    "            valid_emissions = emission_probabilities.get(core_sentence[i],{})\n",
    "\n",
    "            for current_tag in tag_counter:\n",
    "                max_probability = float('-inf')\n",
    "                best_previous_tag = None\n",
    "                for previous_tag in tag_probability_matrix[i-1]:\n",
    "                    probability = tag_probability_matrix[i-1][previous_tag] + transition_probabilities[previous_tag].get(current_tag, EPSILON_LOG)\n",
    "                    probability += valid_emissions.get(current_tag, EPSILON_LOG)\n",
    "                    if probability > max_probability:\n",
    "                        max_probability = probability\n",
    "                        best_previous_tag = previous_tag\n",
    "                tag_probability_matrix[i][current_tag] = max_probability\n",
    "                reminder[i][current_tag] = best_previous_tag\n",
    "        \n",
    "        max_final_probability = float('-inf')\n",
    "        best_final_tag = None\n",
    "        for tag, probability in tag_probability_matrix[len(core_sentence)-1].items():\n",
    "            if  probability> max_final_probability:\n",
    "                max_final_probability = probability\n",
    "                best_final_tag = tag\n",
    "        \n",
    "        best_path = [best_final_tag]\n",
    "        for i in range(len(core_sentence)-1, 0, -1):\n",
    "            best_path.insert(0, reminder[i][best_path[0]])\n",
    "        \n",
    "        tagged_sentence = [(\"START\", \"START\")] + list(zip(core_sentence, best_path)) + [(\"END\", \"END\")]\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frameworks\n",
    "\n",
    "import collections\n",
    "\n",
    "START_TAG = \"START\"\n",
    "END_TAG = \"END\"\n",
    "\n",
    "\n",
    "def evaluate_accuracies(predicted_sentences, tag_sentences):\n",
    "    \"\"\"\n",
    "    :param predicted_sentences:\n",
    "    :param tag_sentences:\n",
    "    :return: (Accuracy, correct word-tag counter, wrong word-tag counter)\n",
    "    \"\"\"\n",
    "    assert len(predicted_sentences) == len(tag_sentences), \"The number of predicted sentence {} does not match the true number {}\".format(len(predicted_sentences), len(tag_sentences))\n",
    "\n",
    "    correct_wordtagcounter = {}\n",
    "    wrong_wordtagcounter = {}\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for pred_sentence, tag_sentence in zip(predicted_sentences, tag_sentences):\n",
    "        assert len(pred_sentence) == len(tag_sentence), \"The predicted sentence length {} does not match the true length {}\".format(len(pred_sentence), len(tag_sentence))\n",
    "        for pred_wordtag, real_wordtag in zip(pred_sentence, tag_sentence):\n",
    "            assert pred_wordtag[0] == real_wordtag[0], \"The predicted sentence WORDS do not match with the original sentence, you should only be predicting the tags\"\n",
    "            word = pred_wordtag[0]\n",
    "            if real_wordtag[1] in [START_TAG, END_TAG]:\n",
    "                continue\n",
    "            if pred_wordtag[1] == real_wordtag[1]:\n",
    "                if word not in correct_wordtagcounter.keys():\n",
    "                    correct_wordtagcounter[word] = collections.Counter()\n",
    "                correct_wordtagcounter[word][real_wordtag[1]] += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                if word not in wrong_wordtagcounter.keys():\n",
    "                    wrong_wordtagcounter[word] = collections.Counter()\n",
    "                wrong_wordtagcounter[word][real_wordtag[1]] += 1\n",
    "                wrong += 1\n",
    "\n",
    "    accuracy = correct / (correct + wrong)\n",
    "\n",
    "    return accuracy, correct_wordtagcounter, wrong_wordtagcounter\n",
    "\n",
    "\n",
    "def specialword_accuracies(train_sentences, predicted_sentences, tag_sentences):\n",
    "    \"\"\"\n",
    "    :param train_sentences:\n",
    "    :param predicted_sentences:\n",
    "    :param tag_sentences:\n",
    "    :return: Accuracy on words with multiple tags, and accuracy on words that do not occur in the training sentences\n",
    "    \"\"\"\n",
    "    seen_words, words_with_multitags_set = get_word_tag_statistics(train_sentences)\n",
    "    multitags_correct = 0\n",
    "    multitags_wrong = 0\n",
    "    unseen_correct = 0\n",
    "    unseen_wrong = 0\n",
    "    for i in range(len(predicted_sentences)):\n",
    "        for j in range(len(predicted_sentences[i])):\n",
    "            word = tag_sentences[i][j][0]\n",
    "            tag = tag_sentences[i][j][1]\n",
    "\n",
    "            if tag in [START_TAG, END_TAG]:\n",
    "                continue\n",
    "\n",
    "            if predicted_sentences[i][j][1] == tag:\n",
    "                if word in words_with_multitags_set:\n",
    "                    multitags_correct += 1\n",
    "                if word not in seen_words:\n",
    "                    unseen_correct += 1\n",
    "            else:\n",
    "                if word in words_with_multitags_set:\n",
    "                    multitags_wrong += 1\n",
    "                if word not in seen_words:\n",
    "                    unseen_wrong += 1\n",
    "    multitag_accuracy = multitags_correct / (multitags_correct + multitags_wrong)\n",
    "    total_unseen = unseen_correct + unseen_wrong\n",
    "    unseen_accuracy = unseen_correct / total_unseen if total_unseen > 0 else 0\n",
    "\n",
    "    return multitag_accuracy, unseen_accuracy\n",
    "\n",
    "\n",
    "def topk_wordtagcounter(wordtagcounter, k):\n",
    "    top_items = sorted(wordtagcounter.items(), key=lambda item: sum(item[1].values()), reverse=True)[:k]\n",
    "    top_items = list(map(lambda item: (item[0], dict(item[1])), top_items))\n",
    "    return top_items\n",
    "\n",
    "\n",
    "def load_dataset(data_file):\n",
    "    if not data_file.endswith(\".txt\"):\n",
    "        raise ValueError(\"File must be a .txt file\")\n",
    "\n",
    "    sentences = []\n",
    "    with open(data_file, 'r', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            sentence = [(START_TAG, START_TAG)]\n",
    "            raw = line.split()\n",
    "            for pair in raw:\n",
    "                splitted = pair.split('=')\n",
    "                if (len(splitted) < 2):\n",
    "                    continue\n",
    "                else:\n",
    "                    tag = splitted[-1]\n",
    "\n",
    "                    # find word\n",
    "                    word = splitted[0]\n",
    "                    for element in splitted[1:-1]:\n",
    "                        word += '/' + element\n",
    "                    sentence.append((word.lower(), tag))\n",
    "            sentence.append((END_TAG, END_TAG))\n",
    "            if len(sentence) > 2:\n",
    "                sentences.append(sentence)\n",
    "            else:\n",
    "                print(sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def strip_tags(sentences):\n",
    "    '''\n",
    "    Strip tags\n",
    "    input:  list of sentences\n",
    "            each sentence is a list of (word,tag) pairs\n",
    "    output: list of sentences\n",
    "            each sentence is a list of words (no tags)\n",
    "    '''\n",
    "\n",
    "    sentences_without_tags = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_without_tags = []\n",
    "        for i in range(len(sentence)):\n",
    "            pair = sentence[i]\n",
    "            sentence_without_tags.append(pair[0])\n",
    "        sentences_without_tags.append(sentence_without_tags)\n",
    "\n",
    "    return sentences_without_tags\n",
    "\n",
    "\n",
    "def get_word_tag_statistics(data_set):\n",
    "    # get set of all seen words and set of words with multitags\n",
    "    word_tags = collections.defaultdict(lambda: set())\n",
    "    word_set = set()\n",
    "    for sentence in data_set:\n",
    "        for word, tag in sentence:\n",
    "            word_tags[word].add(tag)\n",
    "            word_set.add(word)\n",
    "    return word_set, set(map(lambda elem: elem[0], filter(lambda elem: len(elem[1]) > 1, word_tags.items())))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "EPSILON = 1e-5\n",
    "EPSILON_LOG = np.log(EPSILON)\n",
    "\n",
    "def viterbi(train, test):\n",
    "\n",
    "    #1. Count tag, tag pairs, (word, tag) pairs  \n",
    "    initial_counter = Counter() \n",
    "    tag_counter = Counter() \n",
    "    tag_tag_counter = defaultdict(Counter)\n",
    "    word_tag_counter = defaultdict(Counter)\n",
    "\n",
    "    for sentence in train:\n",
    "        initial_counter[sentence[1][1]] += 1\n",
    "        for i in range(len(sentence)-1):\n",
    "            if sentence[i][0] not in [\"START\", \"END\"]:\n",
    "                tag1 = sentence[i][1]\n",
    "                tag2 = sentence[i+1][1]\n",
    "                tag_counter[sentence[i][1]] += 1\n",
    "                word_tag_counter[sentence[i][0]][sentence[i][1]] += 1\n",
    "                tag_tag_counter[tag1][tag2] += 1\n",
    "    \n",
    "    #2. Find each probability and take the log of each probability\n",
    "\n",
    "    #initial probability\n",
    "    total_count_for_initial = sum(initial_counter.values())\n",
    "    initial_probabilities = {tag: np.log(count / total_count_for_initial) \\\n",
    "                             for tag, count in initial_counter.items()}\n",
    "\n",
    "    # Transition counter - P(tagB|tagA)\n",
    "    transition_probabilities = {\n",
    "        prior_tag: {tag: np.log(count / tag_counter[prior_tag]) \\\n",
    "                    for tag, count in tag_counts.items()}\n",
    "        for prior_tag, tag_counts in tag_tag_counter.items()\n",
    "    }\n",
    "\n",
    "    # Emission probabilities - P(word|tag)\n",
    "    emission_probabilities = {\n",
    "        word: {tag: np.log(count / tag_counter[tag]) \\\n",
    "               for tag, count in tag_counts.items()}\n",
    "        for word, tag_counts in word_tag_counter.items() \n",
    "    }\n",
    "\n",
    "    #3. Find the maximum probability path\n",
    "    tagged_sentences = []\n",
    "    for sentence in test:\n",
    "        core_sentence = sentence[1:-1]  \n",
    "        n = len(core_sentence)\n",
    "        tag_probability_matrix = [{} for _ in range(n)]\n",
    "        reminder = [{} for _ in range(n)]\n",
    "        valid_emissions = emission_probabilities.get(core_sentence[0], {})\n",
    "\n",
    "        for tag in tag_counter:\n",
    "            tag_probability_matrix[0][tag] = \\\n",
    "                initial_probabilities.get(tag, EPSILON_LOG) + valid_emissions.get(tag, EPSILON_LOG)\n",
    "\n",
    "        for i in range(1, n):\n",
    "            valid_emissions = emission_probabilities.get(core_sentence[i], {})\n",
    "            if not valid_emissions:\n",
    "                # 무조건 특정 태그로 설정\n",
    "                best_path.append(\"WRONG_TAG\")\n",
    "                continue\n",
    "\n",
    "            previous_tags = tag_probability_matrix[i-1].keys()\n",
    "            for current_tag, emission_probability in valid_emissions.items():\n",
    "                max_probability = float('-inf')\n",
    "                best_previous_tag = None\n",
    "\n",
    "                for previous_tag in previous_tags:\n",
    "                    transition_probability = transition_probabilities.get(previous_tag, {}).get(current_tag, EPSILON_LOG)\n",
    "                    probability = tag_probability_matrix[i-1][previous_tag] + transition_probability + emission_probability\n",
    "\n",
    "                    if probability > max_probability:\n",
    "                        max_probability = probability\n",
    "                        best_previous_tag = previous_tag\n",
    "\n",
    "                tag_probability_matrix[i][current_tag] = max_probability\n",
    "                reminder[i][current_tag] = best_previous_tag\n",
    "\n",
    "        max_final_probability = float('-inf')\n",
    "        best_final_tag = None\n",
    "\n",
    "        for tag, probability in tag_probability_matrix[-1].items():\n",
    "            if probability > max_final_probability:\n",
    "                max_final_probability = probability\n",
    "                best_final_tag = tag\n",
    "\n",
    "        best_path = [best_final_tag]\n",
    "\n",
    "        for i in range(n-1, 0, -1):\n",
    "            best_path.insert(0, reminder[i][best_path[0]])\n",
    "\n",
    "        tagged_sentence = [(\"START\", \"START\")] + list(zip(core_sentence, best_path)) + [(\"END\", \"END\")]\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Viterbi =========\n",
      "time spent: 2.5922 sec\n",
      "accuracy: 0.9310\n",
      "multi-tag accuracy: 0.9402\n",
      "unseen word accuracy: 0.0893\n"
     ]
    }
   ],
   "source": [
    "#viterbi_execution\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import time\n",
    "\n",
    "    train_set = load_dataset('data/brown-training.txt')\n",
    "    dev_set = load_dataset('data/brown-test.txt')\n",
    "\n",
    "    model = viterbi\n",
    "    start_time = time.time()\n",
    "    predicted = model(train_set, strip_tags(dev_set))\n",
    "    time_spend = time.time() - start_time\n",
    "    accuracy, _, _ = evaluate_accuracies(predicted, dev_set)\n",
    "    multi_tag_accuracy, unseen_words_accuracy, = specialword_accuracies(train_set, predicted, dev_set)\n",
    "\n",
    "    print(f\"======== {model.__name__.capitalize()} =========\")\n",
    "    print(\"time spent: {0:.4f} sec\".format(time_spend))\n",
    "    print(\"accuracy: {0:.4f}\".format(accuracy))\n",
    "    print(\"multi-tag accuracy: {0:.4f}\".format(multi_tag_accuracy))\n",
    "    print(\"unseen word accuracy: {0:.4f}\".format(unseen_words_accuracy))\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
